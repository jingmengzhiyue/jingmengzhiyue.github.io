

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Jingmengzhiyue">
  <meta name="keywords" content="">
  
    <meta name="description" content="机器学习算法中的学习理论">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习----学习理论">
<meta property="og:url" content="http://jingmengzhiyue.top/2023/08/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0----%E5%AD%A6%E4%B9%A0%E7%90%86%E8%AE%BA/index.html">
<meta property="og:site_name" content="热爱可抵万难">
<meta property="og:description" content="机器学习算法中的学习理论">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://jingmengzhiyue.top/img/cs229note4f1.png">
<meta property="og:image" content="http://jingmengzhiyue.top/img/cs229note4f2.png">
<meta property="og:image" content="http://jingmengzhiyue.top/img/cs229note4f3.png">
<meta property="og:image" content="http://jingmengzhiyue.top/img/cs229note4f4.png">
<meta property="article:published_time" content="2023-08-01T13:22:28.000Z">
<meta property="article:modified_time" content="2024-03-14T12:20:00.983Z">
<meta property="article:author" content="Jingmengzhiyue">
<meta property="article:tag" content="Machine Learning">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://jingmengzhiyue.top/img/cs229note4f1.png">
  
  
  
  <title>机器学习----学习理论 - 热爱可抵万难</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"jingmengzhiyue.top","root":"/","version":"1.9.5-a","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":"eZ6fNr2wGbxSreDqCc24Ik6y-gzGzoHsz","app_key":"I1HMoNZO5iKmmCSzqE0wTpGr","server_url":"https://ez6fnr2w.lc-cn-n1-shared.com","path":"window.location.pathname","ignore_local":false},"gtag":null},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
  
    <script>
        (function(){
            var bp = document.createElement('script');
            var curProtocol = window.location.protocol.split(':')[0];
            if (curProtocol === 'https') {
                bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
            }
            else {
                bp.src = 'http://push.zhanzhang.baidu.com/push.js';
            }
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(bp, s);
        })();
    </script>
   


<meta name="generator" content="Hexo 6.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>热爱可抵万难</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('https://t.mwm.moe/fj/') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="机器学习----学习理论"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2023-08-01 21:22" pubdate>
          2023年8月1日 晚上
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          14k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          118 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">机器学习----学习理论</h1>
            
            
              <div class="markdown-body">
                
                <h1 id="第四章">第四章</h1>
<h3 id="第六部分-学习理论learning-theory">第六部分 学习理论（Learning
Theory）</h3>
<h4 id="偏差方差的权衡biasvariance-tradeoff">1
偏差/方差的权衡（Bias/variance tradeoff ）</h4>
<p>在讲线性回归的时候，我们讨论过这样的问题：拟合数据的时候，选择线性的“<span
class="math inline">\(y = \theta_0
+\theta_1x\)</span>”这样的“简单”模型，还是选择多项式的“<span
class="math inline">\(y= \theta_0 + \theta_1x+
...+\theta_5x^5\)</span>”这种“复杂”模型。如下图所示：</p>
<p><img src="/img/cs229note4f1.png" srcset="/img/loading.gif" lazyload /></p>
<p>如最右侧图所示，用一个五次多项式来进行拟合，得到的并不是一个好模型。而且，虽然这个五次多项式对于训练集中的每一个
<span
class="math inline">\(x\)</span>（例如之前文中说的居住面积）都给出了非常好的预测的
<span class="math inline">\(y\)</span>
值（对应的就是房屋价格），但是我们也不能指望这个模型能够对训练集之外的点给出靠谱的预测。换句话说，用这种高次多项式来对训练集进行学习得到的模型根本不能扩展运用到其他房屋上面去。一个推测模型（hypothesis）的<strong>泛化误差（generalization
error）</strong>（稍后再给出正式定义），正是那些不属于训练集的样本潜在的预期偏差（expected
error on examples not necessarily in the training set）。</p>
<p>上面图中最左边的线性拟合和最右边的高次多项式拟合都有非常大的泛化误差。然而，这两个模型各自出的问题是很不一样的。如果
<span class="math inline">\(y\)</span> 和 <span
class="math inline">\(x\)</span>
之间的关系不是线性的，那么即便我们有一个非常大规模的训练集，然后用来进行线性拟合，得到的线性模型都还是不能够准确捕捉到数据的结构。我们粗略地将一个模型的<strong>偏差（bias）</strong>
定义为预期的泛化误差（expected generalization
error），即便我们要去拟合的对象是一个非常大的甚至是无限的训练数据集。这样的话，对于上面三幅图中所展示的那个情况来看，最左边的那个线性模型就具有特别大的偏差（bias），可能是对数据欠拟合（也就是说，没有捕捉到数据所体现的结构特征）。</p>
<p>除了这个偏差（bias）之外，还有另外一个构成泛化误差（generalization
error）的因素，也就是模型拟合过程的<strong>方差（variance）。</strong>
例如在最右边的图中，使用了五次多项式进行了拟合，这样有很大的风险，很可能我们基于数据拟合出来的模型可能碰巧只适合于眼下这个小规模的有限的训练集，而并不能反映
<span class="math inline">\(x\)</span> 和 <span
class="math inline">\(y\)</span>
之间更广泛的关系。例如，在实际中，可能我们选择的训练集中的房屋碰巧就是一些比平均价格要稍微贵一些的房屋，也可能有另外的一些比平均值要低一点的房屋，等等。通过对训练集拟合得到的这个“不太靠谱的（spurious）”的模式，我们得到的可能也就是一个有很大泛化误差（large
generalization
error）的模型。这样的话，我们就说这个模型的方差很大（large
variance）<span class="math inline">\(^1\)</span>。</p>
<blockquote>
<p>1
在讲义里面，我们不准备给出对偏差（bias）和方差（variance）给出正式的定义，也就说道上面讨论这样的程度而已。当然了，这两者都有严格的正式定义，例如在线性回归里面，对于这两者的定义，有若干不同的观点，但是哪一个最权威最正确（right）呢？这个还有争议的。</p>
</blockquote>
<p>通常情况下，咱们需要在偏差（bias）和方差（variance）之间进行权衡妥协。如果我们的模型过于“简单（simple）”，而且参数非常少，那这样就可能会有很大的偏差（bias），而方差（variance）可能就很小；如果我们的模型过于“复杂（complex）”，有非常多的参数，那就可能反过来又特别大的方差（variance），而偏差（bias）就会小一些。在上面三种不同拟合的样例中，用二次函数来进行拟合得到的效果，明显是胜过一次线性拟合，也强于五次多项式拟合。</p>
<h4 id="预先准备preliminaries">2 预先准备（Preliminaries）</h4>
<p>在这一部分的讲义中，我们要开始进入到机器学习的理论（learning
theory）了。本章内容非常有趣，而且有启发性，还能帮助我们培养直觉，能够得到在不同背景下如何最佳应用学习算法的经验规则。此外，我们还会探究一些问题：首先，上文我们刚刚谈论到的偏差（bias）/方差（variance），能不能更正规地总结一下？这个问题还会引出关于模型选择的方法，这些方法可以在对一个训练集进行拟合的时候来帮助确定要用的多项式应该是几阶的。其次，在机器学习的过程中，我们真正关注的也就是泛化误差（generalization
error），不过绝大部分的学习算法都是将训练集和模型结合的。那么针对训练集的表现好坏程度，为何就能告诉我们泛化误差的信息呢？例如，我们能将训练集的误差和泛化误差联系起来么？第三个，也是最后一点，是否存在某些条件，我们能否在这些条件下证明某些学习算法能够良好工作？</p>
<p>我们先来给出两个很简单又很有用的引理（lemma）。</p>
<p>引理1 (联合约束，The union bound)。设 <span
class="math inline">\(A_1, A_2, ..., A_k\)</span> 是 <span
class="math inline">\(k\)</span>个不同事件（但不一定互相独立），则有：</p>
<p><span class="math display">\[
P(A_1\cup...\cup A_k)\leq P(A_1)+...+P(A_k)
\]</span></p>
<p>在概率论中，联合约束通常被当做是公理（所以我们就不尝试证明了），实际上也很直观的：
<span class="math inline">\(k\)</span> 个事件至少发生一个的概率最多是
<span class="math inline">\(k\)</span>
个不同的事件各自发生的概率总和。</p>
<p>引理2 (Hoeffding 不等式) 。设 <span
class="math inline">\(Z_1,...,Z_m\)</span> 是 <span
class="math inline">\(m\)</span>
个独立的并且共同遵循伯努利分布（Bernoulli(<span
class="math inline">\(\phi\)</span>)
distribution）的随机变量（independent and identically distributed (iid)
random variables）。例如：<span class="math inline">\(P(Z_i
=1)=\phi\)</span> 而 <span class="math inline">\(P(Z_i =0)= 1 -
\phi\)</span>. 设 <span
class="math inline">\(\hat\phi=(\frac1m)\sum^m_{i=1}Z_i\)</span>
是这些随机变量的平均值，然后设任意的 <span class="math inline">\(\gamma
\geq 0\)</span> 为某一固定值（fixed），则有：</p>
<p><span class="math display">\[
P(|\phi-\hat\phi|&gt;\gamma)\leq 2\exp (-2\gamma^2m)
\]</span></p>
<p>上面这个引理（在机器学习理论里面也称为 <strong>切尔诺夫约束，Chernoff
bound</strong>
）表明，如果我们我们从一个伯努利分布的随机变量中选取平均值 <span
class="math inline">\(\hat\phi\)</span> 来作为对 <span
class="math inline">\(\phi\)</span> 的估计值，那么只要 <span
class="math inline">\(m\)</span>
足够大，我们偏移真实值很远的概率就比较小。另外一种表述方式是：如果你有一个有偏差的硬币（biased
coin），抛起来落下人头朝上的概率是 <span
class="math inline">\(\phi\)</span>，如果你抛了 <span
class="math inline">\(m\)</span> 次，然后计算人头朝上的比例，若 <span
class="math inline">\(m\)</span> 非常大，那么这个比例的值，就是一个对
<span class="math inline">\(\phi\)</span>
的一个概率很高的很好的估计。</p>
<p>基于上面这两个引理，我们就可以去证明在机器学习理论中一些很深刻和重要的结论了。</p>
<p>为了简化表述，我们先集中关注一下二分法分类，其中的标签简化为 <span
class="math inline">\(y \in \{0,
1\}\)</span>。然后我们即将讲到的所有内容也都会推广到其它问题中，例如回归问题以及多类别的分类问题等等。</p>
<p>假设我们有一个给定的训练集 <span class="math inline">\(S =
\{(x^{(i)},y^{(i)});i = 1,...,m\}\)</span>，其样本规模为 <span
class="math inline">\(m\)</span>，集合中的训练样本 <span
class="math inline">\((x^{(i)},y^{(i)})\)</span> 是服从某概率分布 <span
class="math inline">\(D\)</span>
的独立且同分布的随机变量。设一个假设（hypothesis）为<span
class="math inline">\(h\)</span>，我们则用如下的方法定义训练误差（也称为学习理论中的<strong>经验风险
empirical risk</strong> 或者<strong>经验误差 empirical
error）</strong>：</p>
<p><span class="math display">\[
\hat\epsilon(h) =\frac1m\sum^m_{i=1}1\{h(x^{(i)})\neq y^{(i)}\}
\]</span></p>
<p>这个值只是假设模型 <span class="math inline">\(h\)</span>
分类错误样本占据训练样本总数的分数。如果我们要特定指定对某个训练样本集合
<span class="math inline">\(S\)</span> 的经验误差 <span
class="math inline">\(\hat\epsilon(h)\)</span>，可以写作 <span
class="math inline">\(\hat\epsilon_S(h)\)</span>。然后我们就可以定义泛化误差（generalization
error）为：</p>
<p><span class="math display">\[
\epsilon(h) =P_{(x,y)\sim D}(h(x)\neq y)
\]</span></p>
<p>经验误差 <span class="math inline">\(\epsilon(h)\)</span>
的这个定义实际上也就相当于，基于分布 <span
class="math inline">\(D\)</span> 给出的一个新的样本 <span
class="math inline">\((x, y)\)</span> ，假设模型 <span
class="math inline">\(h\)</span> 对该样本分类错误的概率。</p>
<p>要注意，这里我们有一个预先假设，也就是训练集的数据与要用来检验假设用的数据都服从同一个分布
<span
class="math inline">\(D\)</span>（这一假设存在于对泛化误差的定义中）。这个假设通常也被认为是
PAC 假设之一<span class="math inline">\(^2\)</span>。</p>
<blockquote>
<p>2 PAC 是一个缩写，原型为“probably approximately
correct”，这是一个框架和一系列假设的集合，在机器学习理论中的很多结构都是基于这些假设而证明得到的。这个系列假设中<strong>最重要的两个，就是训练集与测试集服从同一分布，以及训练样本的独立性。</strong></p>
</blockquote>
<p>考虑线性分类的情况，假设 <span class="math inline">\(h_\theta (x) =
1\{\theta^T x \geq 0\}\)</span>。拟合参数 <span
class="math inline">\(\theta\)</span>
的合理方法是什么呢？一个思路就是可以使训练误差（training
error）最小化，然后选择取最小值的时候的 <span
class="math inline">\(\theta\)</span> ：</p>
<p><span class="math display">\[
\hat\theta=arg\min_\theta\hat\epsilon(h_\theta)
\]</span></p>
<p>我们把上面这个过程称之为<strong>经验风险最小化</strong>（empirical
risk minimization，缩写为
ERM），而这种情况下通过学习算法得到的假设结果就是 <span
class="math inline">\(\hat h = h_{\hat\theta}\)</span> 。我们把 ERM
看做为最“基础（basic）”的学习算法，在这一系列的讲义中我们主要关注的就是这种算法。（其他的例如逻辑回归等等算法也可以看作是对
ERM 的某种近似（approximations）。）</p>
<p>在咱们关于机器学习理论的研究中，有一种做法很有用处，就是把具体的参数化（specific
parameterization）抽象出去，并且也把是否使用线性分选器（linear
classifier）之类的问题也抽象出去。我们把通过学习算法所使用的<strong>假设类（hypothesis
class）<span class="math inline">\(H\)</span></strong>
定义为所有分类器的集合（set of all
classifiers）。对于线性分类问题来说，<span class="math inline">\(H =
\{h_\theta : h_\theta(x) = 1\{\theta^T x \geq 0\}, \theta \in
R^{n+1}\}\)</span>，是一个对 <span
class="math inline">\(X\)</span>（输入特征）
进行分类的所有分类器的集合，其中所有分类边界为线性。更广泛来说，假设我们研究神经网络（neural
networks），那么可以设 <span class="math inline">\(H\)</span>
为能表示某些神经网络结构的所有分类器的集合。</p>
<p>现在就可以把 经验风险最小化（ERM）看作是对函数类 <span
class="math inline">\(H\)</span>
的最小化，其中由学习算法来选择假设（hypothesis）：</p>
<p><span class="math display">\[
\hat h=arg\min_{h\in H}\hat\epsilon(h)
\]</span></p>
<h4 id="有限个假设finite-h的情况">3 有限个假设（finite H）的情况</h4>
<p>我们首先来考虑一下假设类有限情况下的学习问题，其中假设类 <span
class="math inline">\(H = \{h_1, ..., h_k\}\)</span>，由 <span
class="math inline">\(k\)</span> 个不同假设组成。因此，<span
class="math inline">\(H\)</span> 实际上就是由 <span
class="math inline">\(k\)</span> 个从输入特征 <span
class="math inline">\(X\)</span> 映射到 <span class="math inline">\(\{0,
1\}\)</span> 的函数组成的集合，而经验风险最小化（ERM）就是从这样的 <span
class="math inline">\(k\)</span> 个函数中选择训练误差最小（smallest
training error）的作为 <span class="math inline">\(\hat h\)</span>。</p>
<p>我们希望能够确保 <span class="math inline">\(\hat{h}\)</span>
的泛化误差。这需要两个步骤：首先要表明 <span
class="math inline">\(\hat\epsilon(h)\)</span> 是对所有 <span
class="math inline">\(h\)</span> 的 <span
class="math inline">\(\epsilon(h)\)</span>
的一个可靠估计。其次就需要表明这个 <span
class="math inline">\(\hat\epsilon(h)\)</span> 位于 <span
class="math inline">\(\hat{h}\)</span> 泛化误差的上界。</p>
<p>任选一个固定的 <span class="math inline">\(h_i \in
H\)</span>。假如有一个伯努利随机变量（Bernoulli random variable） <span
class="math inline">\(Z\)</span>，其分布入下面式中定义。 然后我们从
<span class="math inline">\(D\)</span> 中取样 <span
class="math inline">\((x, y)\)</span>，并设 <span
class="math inline">\(Z=1\{h_i(x)\neq
y\}\)</span>。也就是说，我们会选择一个样本，然后令 <span
class="math inline">\(Z\)</span> 指示 <span
class="math inline">\(h_i\)</span>
是否对该样本进行了错误分类。类似地，我们还定义了一个 <span
class="math inline">\(Z_j=1\{h_i(x^{(j)})\neq
y^{(j)}\}\)</span>。由于我们的训练样本都是从 <span
class="math inline">\(D\)</span>
中取来的独立随机变量（iid），所以在此基础上构建的 <span
class="math inline">\(Z\)</span> 和 <span
class="math inline">\(Z_j\)</span> 也都服从相同的分布。</p>
<p>这样就能找到针对随机选取的训练样本进行错误分类的概率 — 也就是 <span
class="math inline">\(\epsilon(h)\)</span> — 正好就是 <span
class="math inline">\(Z\)</span> (以及 <span
class="math inline">\(Z_j\)</span>) 的期望值（expected
value）。然后，就可以把训练误差写成下面这种形式：</p>
<p><span class="math display">\[
\hat\epsilon(h_i)=\frac 1m\sum_{j=1}^mZ_j
\]</span></p>
<p>因此，<span class="math inline">\(\hat\epsilon(h_i)\)</span> 就正好是
<span class="math inline">\(m\)</span> 个随机变量 <span
class="math inline">\(Z_j\)</span> 的平均值，而这个 <span
class="math inline">\(Z_j\)</span>
是服从伯努利分布的独立随机变量（iid），其均值就是 <span
class="math inline">\(\epsilon(h_i)\)</span>。接下来，就可以使用
Hoeffding 不等式，得到下面的式子：</p>
<p><span class="math display">\[
P(|\epsilon(h_i)-\hat\epsilon(h_i)|&gt;\gamma)\leq 2\exp (-2\gamma^2m)
\]</span></p>
<p>这就表明，对于我们给定的某个固定的 <span
class="math inline">\(h_i\)</span>，假如训练样本的规模 <span
class="math inline">\(m\)</span>
规模很大的时候，训练误差有很接近泛化误差（generalization
error）的概率是很高的。然而我们不仅仅满足于针对某一个特定的 <span
class="math inline">\(h_i\)</span> 的时候能保证 <span
class="math inline">\(\epsilon(h_i)\)</span> 接近 <span
class="math inline">\(\hat\epsilon(h_i)\)</span>
且接近的概率很高。我们还要证明同时针对所有的 <span
class="math inline">\(h \in H\)</span> 这个结论都成立。
为了证明这个结论，我们设 <span class="math inline">\(A_i\)</span>
来表示事件 <span class="math inline">\(|\epsilon(h_i) -
\hat\epsilon(h_i)| &gt; \gamma\)</span>。我们已经证明了，对于任意给定的
<span class="math inline">\(A_i\)</span>，都有 <span
class="math inline">\(P(A_i) \le 2\exp (-2\gamma^2m)\)</span>
成立。接下来，使用联合约束（union bound），就可以得出下面的关系：</p>
<p><span class="math display">\[
\begin{aligned}
P(\exists h\in H.|\epsilon(h_i)-\hat\epsilon(h_i)|&gt;\gamma)&amp; =
P(A_1\cup...\cup A_k) \\
                                                   &amp; \le
\sum_{i=1}^k P(A_i) \\
                                                   &amp; \le
\sum_{i=1}^k 2\exp (-2\gamma^2m) \\
                                                   &amp; = 2k\exp
(-2\gamma^2m)
\end{aligned}
\]</span></p>
<p>如果等式两边都用 1 来减去原始值（subtract both sides from
1），则不等关系改变为：</p>
<p><span class="math display">\[
\begin{aligned}
P(\neg\exists h\in H.|\epsilon(h_i)-\hat\epsilon(h_i)|&gt;\gamma)&amp;
= P(\forall h\in H.|\epsilon(h_i)-\hat\epsilon(h_i)|\le\gamma) \\
&amp; \ge 1-2k\exp (-2\gamma^2m)
\end{aligned}
\]</span></p>
<p>（<span class="math inline">\(“\neg”\)</span> 这个符号的意思是
“非”。）如上所示，至少有 <span class="math inline">\(1-2k exp(-2\gamma^2
m)\)</span> 的概率，我们能确保对于所有的 <span class="math inline">\(h
\in H\)</span>，<span class="math inline">\(\epsilon(h)\)</span> 在
<span class="math inline">\(\hat\epsilon(h)\)</span> 附近的 <span
class="math inline">\(\gamma\)</span>
范围内。这种结果就叫做一致收敛结果（uniform convergence
result），因为这是一个针对所有的 <span class="math inline">\(h \in
H\)</span> 都同时成立的约束（与之相反的是只针对某一个 <span
class="math inline">\(h\)</span> 才成立的情况）。</p>
<p>在上面的讨论中，我们涉及到的是针对某些 <span
class="math inline">\(m\)</span> 和 <span
class="math inline">\(\gamma\)</span>
的特定值，给定一个概率约束：对于某些 <span class="math inline">\(h \in
H\)</span>, 都有 <span class="math inline">\(|\epsilon(h) -
\hat\epsilon(h)| \geq \gamma\)</span>。这里我们感兴趣的变量（quantities
of interest）有三个：<span class="math inline">\(m\)</span>, <span
class="math inline">\(\gamma\)</span>, 以及误差的概率（probability of
error）；我们可以将其中的任意一个用另外两个来进行约束（bound either one
in terms of the other two）。</p>
<p>例如，我们可以提出下面这样的一个问题：给定一个 <span
class="math inline">\(\gamma\)</span> 以及某个 <span
class="math inline">\(\delta \geq
0\)</span>，那么如果要保证训练误差处于泛化误差附近 <span
class="math inline">\(\gamma\)</span> 的范围内的概率最小为 <span
class="math inline">\(1 – \delta\)</span>，那么 <span
class="math inline">\(m\)</span> 应该要多大呢？可以设 <span
class="math inline">\(\delta = 2k exp(-2\gamma^2 m)\)</span> 然后解出来
<span
class="math inline">\(m\)</span>（自己给自己证明一下这样是对的吧！），然后我们就发现，如果有：</p>
<p><span class="math display">\[
m\ge \frac{1}{2\gamma^2}log\frac{2k}{\delta}
\]</span></p>
<p>并且概率最小为 <span
class="math inline">\(1-\delta\)</span>，就能保证对于所有的 <span
class="math inline">\(h \in H\)</span> 都有 <span
class="math inline">\(|\epsilon(h) - \hat\epsilon(h)| ≤ \gamma\)</span>
。（反过来，这也表明，对于<strong>某些</strong> <span
class="math inline">\(h \in H\)</span>， <span
class="math inline">\(|\epsilon(h) - \hat\epsilon(h)| \geq
\gamma\)</span> 的概率最大为 <span
class="math inline">\(\delta\)</span>。）这种联合约束也说明了需要多少数量的训练样本才能对结果有所保证。是某些特定的方法或者算法所需要训练集的规模
<span class="math inline">\(m\)</span> 来实现一定程度的性能（achieve a
certain level of performance），这样的训练集规模 <span
class="math inline">\(m\)</span>
也叫做此类算法的<strong>样本复杂度</strong> （the algorithm’s sample
complexity）。</p>
<p>上面这个约束的关键特性在于要保证结果，所需的训练样本数量只有 <span
class="math inline">\(k\)</span> 的对数（only logarithmic in k），<span
class="math inline">\(k\)</span> 即假设集合 <span
class="math inline">\(H\)</span>
中的假设个数。这以特性稍后会很重要。</p>
<p>同理，我们也可以将 <span class="math inline">\(m\)</span> 和 <span
class="math inline">\(\delta\)</span> 设置为固定值，然后通过上面的等式对
<span class="math inline">\(\gamma\)</span> 进行求解，然后表明对于所有的
<span class="math inline">\(h \in H\)</span> ，都有概率为 <span
class="math inline">\(1
–\delta\)</span>（这里还是要你自己去证明了，不过你相信这个是对的就好了。）。</p>
<p><span class="math display">\[
|\hat\epsilon(h)-\epsilon(h)|\le \sqrt{\frac{1}{2m}log\frac{2k}{\delta}}
\]</span></p>
<p>现在，我们假设这个联合收敛成立（uniform convergence
holds），也就是说，对于所有的 <span class="math inline">\(h \in
H\)</span>，都有 <span class="math inline">\(|ε(h)-\hat\epsilon(h)| ≤
\gamma\)</span>。我们的学习算法选择了 <span
class="math inline">\(\hat{h} = arg\min_{h\in H}
\hat\epsilon(h)\)</span>，关于这种算法的泛化，我们能给出什么相关的证明呢？</p>
<p>将 <span class="math inline">\(h^∗ = arg \min_{h\in H}
\epsilon(h)\)</span> 定义为 <span class="math inline">\(H\)</span>
中最佳可能假设（best possible hypothesis）。这里要注意此处的 <span
class="math inline">\(h^∗\)</span> 是我们使用假设集合 <span
class="math inline">\(H\)</span>
所能找出的最佳假设，所以很自然地，我们就能理解可以用这个 <span
class="math inline">\(h^∗\)</span> 来进行性能对比了。则有：</p>
<p><span class="math display">\[
\begin{aligned}
\epsilon(\hat h) &amp; \le \hat\epsilon(\hat h)+\gamma \\
                 &amp; \le \hat\epsilon(h^*)+\gamma \\
                 &amp; \le \epsilon(h^*)+2\gamma
\end{aligned}
\]</span></p>
<p>上面的第一行用到了定理 <span class="math inline">\(| \epsilon(\hat h)
- \hat\epsilon (\hat h) | \le
\gamma\)</span>（可以通过上面的联合收敛假设来推出）。第二行用到的定理是
<span class="math inline">\(\hat{h}\)</span> 是选来用于得到最小 <span
class="math inline">\(\hat\epsilon(h)\)</span> ，然后因此对于所有的
<span class="math inline">\(h\)</span> 都有 <span
class="math inline">\(\hat\epsilon(\hat{h}) \leq
\hat\epsilon(h)\)</span>，也就自然能推出 <span
class="math inline">\(\hat\epsilon(\hat{h}) \le
\hat\epsilon(h^∗)\)</span>
。第三行再次用到了上面的联合收敛假设，此假设表明 <span
class="math inline">\(\hat\epsilon(h^∗) \le \epsilon(h^∗) +
\gamma\)</span>
。所以，我们就能得出下面这样的结论：如果联合收敛成立，那么 <span
class="math inline">\(\hat h\)</span> 的泛化误差最多也就与 <span
class="math inline">\(H\)</span> 中的最佳可能假设相差 <span
class="math inline">\(2\gamma\)</span>。</p>
<p>好了，咱们接下来就把上面这一大堆整理成一条定理（theorem）。</p>
<p><strong>定理:</strong> 设 <span class="math inline">\(|H| =
k\)</span><code>译者注：即 H 集合中元素个数为 k</code>，然后设 <span
class="math inline">\(m\)</span> 和 <span
class="math inline">\(\delta\)</span> 为任意的固定值。然后概率至少为
<span class="math inline">\(1 - \delta\)</span>，则有：</p>
<p><span class="math display">\[
\epsilon(\hat h)\le (\min_{h\in
H}\epsilon(h))+2\sqrt{\frac{1}{2m}log\frac{2k}{\delta}}
\]</span></p>
<p>上面这个的证明，可以通过令 <span
class="math inline">\(\gamma\)</span> 等于平方根<span
class="math inline">\(\sqrt{\cdot}\)</span>的形式，然后利用我们之前得到的概率至少为
<span class="math inline">\(1 – \delta\)</span>
的情况下联合收敛成立，接下来利用联合收敛能表明 <span
class="math inline">\(\epsilon(h)\)</span> 最多比 <span
class="math inline">\(\epsilon(h^∗) = \min_{h\in H} \epsilon(h)\)</span>
多 <span
class="math inline">\(2\gamma\)</span>（这个前面我们已经证明过了）。</p>
<p>这也对我们之前提到过的在模型选择的过程中在偏差（bias）/方差（variance）之间的权衡给出了定量方式。例如，加入我们有某个假设类
<span
class="math inline">\(H\)</span>，然后考虑切换成某个更大规模的假设类
<span class="math inline">\(H&#39; \supseteq
H\)</span>。如果我们切换到了 <span class="math inline">\(H&#39;\)</span>
，那么第一次的 <span class="math inline">\(\min_h \epsilon(h)\)</span>
只可能降低（因为我们这次在一个更大规模的函数集合里面来选取最小值了）。因此，使用一个更大规模的假设类来进行学习，我们的学习算法的“偏差（bias）”只会降低。然而，如果
<span class="math inline">\(k\)</span>
值增大了，那么第二项的那个二倍平方根项<span
class="math inline">\(2\sqrt{\cdot}\)</span>也会增大。这一项的增大就会导致我们使用一个更大规模的假设的时候，“方差（variance）”就会增大。</p>
<p>通过保持 <span class="math inline">\(\gamma\)</span> 和 <span
class="math inline">\(\delta\)</span> 为固定值，然后像上面一样求解 <span
class="math inline">\(m\)</span>，我们还能够得到下面的样本复杂度约束：</p>
<p><strong>推论（Corollary）：</strong> 设 <span
class="math inline">\(|H| = k\)</span> ，然后令 <span
class="math inline">\(\delta,\gamma\)</span>
为任意的固定值。对于满足概率最少为 <span class="math inline">\(1 -
\delta\)</span> 的 <span class="math inline">\(\epsilon(\hat{h}) \le
min_{h\in H} \epsilon(h) + 2\gamma\)</span> ，下面等式关系成立：</p>
<p><span class="math display">\[
\begin{aligned}
m &amp;\ge \frac{1}{2\gamma^2}log\frac{2k}{\delta} \\
  &amp; = O(\frac{1}{\gamma^2}log\frac{k}{\delta})    
\end{aligned}
\]</span></p>
<h4 id="无限个假设infinite-h的情况">4 无限个假设（infinite
H）的情况</h4>
<p>我们已经针对有限个假设类的情况证明了一些有用的定理。然而有很多的假设类都包含有无限个函数，其中包括用实数参数化的类（比如线性分类问题）。那针对这种无限个假设的情况，我们能证明出类似的结论么？</p>
<p>我们先从一些不太“准确”论证的内容开始（not the “right”
argument）。当然也有更好的更通用的论证，但先从这种不太“准确”的内容除法，将有助于锻炼我们在此领域内的直觉（intuitions
about the domain）。</p>
<p>若我们有一个假设集合 <span class="math inline">\(H\)</span>，使用
<span class="math inline">\(d\)</span> 个实数来进行参数化（parameterized
by d real numbers）。由于我们使用计算机表述实数，而 IEEE
的双精度浮点数（ C 语言里面的 double 类型）使用了 64 bit
来表示一个浮点数（floating-point
number,），这就意味着如果我们在学习算法中使用双精度浮点数（double-
precision floating point），那我们的算法就由 64 d 个 bit
来进行参数化（parameterized by 64d
bits）。这样我们的这个假设类实际上包含的不同假设的个数最多为 <span
class="math inline">\(k = 2^{64d}\)</span>
。结合上一节的最后一段那个推论（Corollary），我们就能发现，要保证 <span
class="math inline">\(\epsilon(\hat{h}) \leq \epsilon(h^∗) +
2\gamma\)</span> ，同时还要保证概率至少为 <span class="math inline">\(1
- \delta\)</span> ，则需要训练样本规模 <span
class="math inline">\(m\)</span> 满足<span class="math inline">\(m \ge
O(\frac{1}{\gamma^2}log\frac{2^{64d}}{\delta})=O(\frac{d}{\gamma^2}log\frac{1}{\delta})=O_{\gamma,\delta}(d)\)</span>（这里的
<span class="math inline">\(\gamma\)</span>，<span
class="math inline">\(\delta\)</span>下标表示最后一个大<span
class="math inline">\(O\)</span>可能是一个依赖于<span
class="math inline">\(\gamma\)</span>和<span
class="math inline">\(\delta\)</span>的隐藏常数。）因此，所需的训练样本规模在模型参数中最多也就是线性的（the
number of training examples needed is at most linear in the parameters
of the model）。</p>
<p>The fact that we relied on 64-bit floating point makes this argument
not entirely satisfying, but the conclusion is nonetheless roughly
correct: If what we’re going to do is try to minimize training error,
then in order to learn “well” using a hypothesis class that has d
parameters, generally we’re going to need on the order of a linear
number of training examples in d.</p>
<p>上述论证依赖于假定参数是 64
位浮点数（但是实际上实数参数不一定如此实现），因此还不能完全令人满意，但这个结论大致上是正确的：如果我们试图使训练误差（training
error）最小化，那么为了使用具有 <span class="math inline">\(d\)</span>
个参数的假设类（hypothesis class）的学习效果“较好（well）”，通常就需要
<span class="math inline">\(d\)</span> 的线性规模个训练样本。</p>
<p>（在这里要注意的是，对于使用经验风险最小化（empirical risk
minimization
，ERM）的学习算法，上面这些结论已经被证明适用。因此，样本复杂度（sample
complexity）对 <span class="math inline">\(d\)</span>
的线性依赖性通常适用于大多数分类识别学习算法（discriminative learning
algorithms），但训练误差或者训练误差近似值的最小化，就未必适用于分类识别了。对很多的非
ERM 学习算法提供可靠的理论论证，仍然是目前很活跃的一个研究领域。）</p>
<p>前面的论证还有另外一部分让人不太满意，就是依赖于对 <span
class="math inline">\(H\)</span>
的参数化（parameterization）。根据直觉来看，这个参数化似乎应该不会有太大影响：我们已经把线性分类器（linear
classifiers）写成了 <span class="math inline">\(h_\theta(x) =
1\{\theta_0 + \theta_1x_1 + ···\theta_n x_n \geq 0\}\)</span>
的形式，其中有 <span class="math inline">\(n+1\)</span> 个参数 <span
class="math inline">\(\theta_0,...,\theta_n\)</span> 。但也可以写成
<span class="math inline">\(h_{u,v}(x) = 1\{(u^2_0 - v_0^2) + (u^2_1 -
v_1^2)x1 + ··· (u^2_n - v_n^2)x_n \geq 0\}\)</span> 的形式，这样就有
<span class="math inline">\(2n+2\)</span> 个参数 <span
class="math inline">\(u_i, v_i\)</span>
了。然而这两种形式都定义了同样的一个 <span
class="math inline">\(H：\)</span> 一个 <span
class="math inline">\(n\)</span> 维的线性分类器集合。</p>
<p>要推导出更让人满意的论证结果，我们需要再额外定义一些概念。</p>
<p>给定一个点的集合 <span class="math inline">\(S = \{x^{(i)}, ...,
x^{(d)}\}\)</span>（与训练样本集合无关），其中 <span
class="math inline">\(x(i) \in X\)</span>，如果 <span
class="math inline">\(H\)</span> 能够对 集合 <span
class="math inline">\(S\)</span> 实现任意的标签化（can realize any
labeling on S），则称 <strong><span class="math inline">\(H\)</span>
打散（shatter）</strong> 了 <span
class="math inline">\(S\)</span>。例如，对于任意的标签集合 （set of
labels）<span class="math inline">\(\{y^{(1)}, ...,
y^{(d)}\}\)</span>，都有 一些<span class="math inline">\(h\in H\)</span>
，对于所有的<span class="math inline">\(i = 1, ...d\)</span>，式子<span
class="math inline">\(h(x^{(i)}) =
y^{(i)}\)</span>都成立。（译者注：关于 shattered set 的定义可以参考：<a
target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Shattered_set"><em>https://en.wikipedia.org/wiki/Shattered_set</em></a>
更多关于 VC 维 的内容也可以参考：<a
target="_blank" rel="noopener" href="https://www.zhihu.com/question/38607822"><em>https://www.zhihu.com/question/38607822</em></a>
）</p>
<p>给定一个假设类 <span class="math inline">\(H\)</span>，我们定义其
<strong>VC维度（Vapnik-Chervonenkis dimension），</strong> 写作 <span
class="math inline">\(VC(H)\)</span>，这个值也就是能被 <span
class="math inline">\(H\)</span> 打散（shatter）的最大的集合规模。（如果
<span class="math inline">\(H\)</span> 能打散任意大的集合（arbitrarily
large sets），那么 <span class="math inline">\(VC(H) =
∞\)</span>。）</p>
<p>例如，若一个集合由下图所示的三个点组成：</p>
<p><img src="/img/cs229note4f2.png" srcset="/img/loading.gif" lazyload /></p>
<p>那么二维线性分类器 <span class="math inline">\((h(x) = 1\{\theta_0
+\theta_1 x_1 + \theta_2 x_2 \geq 0\})\)</span> 的集合 <span
class="math inline">\(H\)</span>
能否将上图所示的这个集合打散呢？答案是能。具体来看则如下图所示，以下八种分类情况中的任意一个，我们都能找到一种用能够实现
“零训练误差（zero training error）” 的线性分类器（linear
classifier）：</p>
<p><img src="/img/cs229note4f3.png" srcset="/img/loading.gif" lazyload /></p>
<p>此外，这也有可能表明，这个假设类 <span
class="math inline">\(H\)</span> 不能打散（shatter）4
个点构成的集合。因此，<span class="math inline">\(H\)</span>
可以打散（shatter）的最大集合规模为 3，也就是说 <span
class="math inline">\(VC（H）= 3\)</span>。</p>
<p>这里要注意，<span class="math inline">\(H\)</span> 的 <span
class="math inline">\(VC\)</span> 维 为3，即便有某些 3 个点的集合不能被
<span class="math inline">\(H\)</span>
打散。例如如果三个点都在一条直线上（如下图左侧的图所示），那就没办法能够用线性分类器来对这三个点的类别进行划分了（如下图右侧所示）。</p>
<p><img src="/img/cs229note4f4.png" srcset="/img/loading.gif" lazyload /></p>
<p>换个方式来说，在 <span class="math inline">\(VC\)</span> 维
的定义之下，要保证 <span class="math inline">\(VC(H)\)</span> 至少为
<span class="math inline">\(d\)</span>，只需要证明至少有一个规模为 <span
class="math inline">\(d\)</span> 的集合能够被 <span
class="math inline">\(H\)</span> 打散 就可以了。</p>
<p>这样就能够给出下面的定理（theorem）了，该定理来自
Vapnik。（有不少人认为这是所有学习理论中最重要的一个定理。）</p>
<p>Theorem. Let H be given, and let d = VC(H). Then with probability at
least 1-δ, we have that for all h∈H,</p>
<p>定理：给定 <span class="math inline">\(H\)</span>，设 <span
class="math inline">\(d = VC(H)\)</span>。然后对于所有的 <span
class="math inline">\(h\in H\)</span>，都有至少为 <span
class="math inline">\(1-\delta\)</span> 的概率使下面的关系成立：</p>
<p><span class="math display">\[
|\epsilon(h)-\hat\epsilon(h)|\le O(\sqrt{\frac{d}{m}log\frac{d}{m}+\frac
1mlog\frac 1\delta})
\]</span></p>
<p>此外，有至少为 <span class="math inline">\(1-\delta\)</span>
的概率：</p>
<p><span class="math display">\[
\epsilon(\hat h)\le
\epsilon(h^*)+O(\sqrt{\frac{d}{m}log\frac{d}{m}+\frac 1mlog\frac
1\delta})
\]</span></p>
<p>换句话说，如果一个假设类有有限的 <span
class="math inline">\(VC\)</span> 维，那么只要训练样本规模 <span
class="math inline">\(m\)</span> 增大，就能够保证联合收敛成立（uniform
convergence occurs）。和之前一样，这就能够让我们以 <span
class="math inline">\(\epsilon(h)\)</span> 的形式来给 <span
class="math inline">\(\epsilon(h^∗)\)</span>
建立一个约束（bound）。此外还有下面的推论（corollary）：</p>
<p>Corollary. For <span class="math inline">\(|ε(h) - \hat\epsilon(h)| ≤
\gamma\)</span> to hold for all h ∈ H (and hence <span
class="math inline">\(\epsilon(\hat{h}) ≤ \epsilon(h^∗) +
2\gamma\)</span>) with probability at least 1 - δ, it suffices that
<span class="math inline">\(m = O_{\gamma,\delta}(d)\)</span>.</p>
<p><strong>推论（Corollary）：</strong> 对于所有的 <span
class="math inline">\(h \in H\)</span> 成立的 <span
class="math inline">\(|\epsilon(h) - \epsilon(\hat h)| \le
\gamma\)</span> （因此也有 <span class="math inline">\(\epsilon(\hat h)
≤ \epsilon(h^∗) + 2\gamma\)</span>），则有至少为 <span
class="math inline">\(1 – \delta\)</span> 的概率，满足 <span
class="math inline">\(m = O_{\gamma,\delta}(d)\)</span>。</p>
<p>In other words, the number of training examples needed to learn
“well” using H is linear in the VC dimension of H. It turns out that,
for “most” hypothesis classes, the VC dimension (assuming a “reasonable”
parameterization) is also roughly linear in the number of parameters.
Putting these together, we conclude that (for an algorithm that tries to
minimize training error) the number of training examples needed is
usually roughly linear in the number of parameters of H.</p>
<p>换个方式来说，要保证使用 假设集合 <span
class="math inline">\(H\)</span> 的
机器学习的算法的学习效果“良好（well）”，那么训练集样本规模 <span
class="math inline">\(m\)</span> 需要与 <span
class="math inline">\(H\)</span> 的 <span
class="math inline">\(VC\)</span> 维度 线性相关（linear in the VC
dimension of
H）。这也表明，对于“绝大多数（most）”假设类来说，（假设是“合理（reasonable）”参数化的）<span
class="math inline">\(VC\)</span>
维度也大概会和参数的个数线性相关。把这些综合到一起，我们就能得出这样的一个结论：对于一个试图将训练误差最小化的学习算法来说：训练样本个数
通常都大概与假设类 <span class="math inline">\(H\)</span> 的参数个数
线性相关。</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/Machine-Learning/" class="category-chain-item">Machine Learning</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/Machine-Learning/" class="print-no-link">#Machine Learning</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>机器学习----学习理论</div>
      <div>http://jingmengzhiyue.top/2023/08/01/机器学习----学习理论/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Jingmengzhiyue</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2023年8月1日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2023/10/01/%E7%AC%AC%E5%9B%9B%E7%AB%A0%20%E5%88%86%E7%BB%84/" title="Pandas分组">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">Pandas分组</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2023/08/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0----%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" title="机器学习----支持向量机">
                        <span class="hidden-mobile">机器学习----支持向量机</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div id="valine"></div>
  <script type="text/javascript">
    Fluid.utils.loadComments('#valine', function() {
      Fluid.utils.createScript('https://lib.baomitu.com/valine/1.5.1/Valine.min.js', function() {
        var options = Object.assign(
          {"appId":"eZ6fNr2wGbxSreDqCc24Ik6y-gzGzoHsz","appKey":"I1HMoNZO5iKmmCSzqE0wTpGr","path":"window.location.pathname","placeholder":null,"avatar":"retro","meta":["nick","mail","link"],"requiredFields":[],"pageSize":10,"lang":"zh-CN","highlight":false,"recordIP":false,"serverURLs":"","emojiCDN":null,"emojiMaps":null,"enableQQ":false},
          {
            el: "#valine",
            path: window.location.pathname
          }
        )
        new Valine(options);
        Fluid.utils.waitElementVisible('#valine .vcontent', () => {
          var imgSelector = '#valine .vcontent img:not(.vemoji)';
          Fluid.plugins.imageCaption(imgSelector);
          Fluid.plugins.fancyBox(imgSelector);
        })
      });
    });
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  





  <script>
  Fluid.utils.createScript('https://lib.baomitu.com/mermaid/8.14.0/mermaid.min.js', function() {
    mermaid.initialize({"theme":"default"});

    Fluid.utils.listenDOMLoaded(function() {
      Fluid.events.registerRefreshCallback(function() {
        if ('mermaid' in window) {
          mermaid.init();
        }
      });
    });
  });
</script>






    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="leancloud-site-pv-container" style="display: none">
        总访问量 18954 
        <span id="leancloud-site-pv"></span>
         次
      </span>
    
    
      <span id="leancloud-site-uv-container" style="display: none">
        总访客数 9873 
        <span id="leancloud-site-uv"></span>
         人
      </span>
    
    

  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="/js/leancloud.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
